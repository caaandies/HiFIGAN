{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "import hydra\n",
    "import torch\n",
    "from hydra.utils import instantiate\n",
    "from omegaconf import OmegaConf\n",
    "\n",
    "from src.datasets.data_utils import get_dataloaders\n",
    "from src.trainer import Trainer\n",
    "from src.utils.init_utils import set_random_seed, setup_saving_and_logging\n",
    "\n",
    "from src.transforms import MelSpectrogramConfig, MelSpectrogram\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "\n",
    "\n",
    "@hydra.main(version_base=None, config_path=\"src/configs\", config_name=\"train\")\n",
    "def main(config):\n",
    "    \"\"\"\n",
    "    Main script for training. Instantiates the model, optimizer, scheduler,\n",
    "    metrics, logger, writer, and dataloaders. Runs Trainer to train and\n",
    "    evaluate the model.\n",
    "\n",
    "    Args:\n",
    "        config (DictConfig): hydra experiment config.\n",
    "    \"\"\"\n",
    "    set_random_seed(config.trainer.seed)\n",
    "\n",
    "    project_config = OmegaConf.to_container(config)\n",
    "    logger = setup_saving_and_logging(config)\n",
    "    writer = instantiate(config.writer, logger, project_config)\n",
    "\n",
    "    if config.trainer.device == \"auto\":\n",
    "        device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    else:\n",
    "        device = config.trainer.device\n",
    "\n",
    "    spec_transform = instantiate(config.spec_transform)\n",
    "\n",
    "    # setup data_loader instances\n",
    "    # batch_transforms should be put on device\n",
    "    dataloaders = get_dataloaders(config, device)\n",
    "\n",
    "    # build model architecture, then print to console\n",
    "    model = instantiate(config.model).to(device)\n",
    "    logger.info(model)\n",
    "\n",
    "    # get function handles of loss and metrics\n",
    "    loss_function = instantiate(config.loss_function).to(device)\n",
    "    metrics = instantiate(config.metrics)\n",
    "\n",
    "    # build optimizer, learning rate scheduler\n",
    "    trainable_mpd_params = filter(lambda p: p.requires_grad, model.mpd.parameters())\n",
    "    mpd_optimizer = instantiate(config.mpd_optimizer, params=trainable_mpd_params)\n",
    "\n",
    "    trainable_msd_params = filter(lambda p: p.requires_grad, model.msd.parameters())\n",
    "    msd_optimizer = instantiate(config.msd_optimizer, params=trainable_msd_params)\n",
    "\n",
    "    trainable_gen_params = filter(lambda p: p.requires_grad, model.gen.parameters())\n",
    "    gen_optimizer = instantiate(config.gen_optimizer, params=trainable_gen_params)\n",
    "\n",
    "    mpd_lr_scheduler = instantiate(config.mpd_lr_scheduler, optimizer=mpd_optimizer)\n",
    "    msd_lr_scheduler = instantiate(config.msd_lr_scheduler, optimizer=msd_optimizer)\n",
    "    gen_lr_scheduler = instantiate(config.gen_lr_scheduler, optimizer=gen_optimizer)\n",
    "\n",
    "    # epoch_len = number of iterations for iteration-based training\n",
    "    # epoch_len = None or len(dataloader) for epoch-based training\n",
    "    epoch_len = config.trainer.get(\"epoch_len\")\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        spec_transform=spec_transform,\n",
    "        criterion=loss_function,\n",
    "        metrics=metrics,\n",
    "        mpd_optimizer=mpd_optimizer,\n",
    "        msd_optimizer=msd_optimizer,\n",
    "        gen_optimizer=gen_optimizer,\n",
    "        mpd_lr_scheduler=mpd_lr_scheduler,\n",
    "        msd_lr_scheduler=msd_lr_scheduler,\n",
    "        gen_lr_scheduler=gen_lr_scheduler,\n",
    "        config=config,\n",
    "        device=device,\n",
    "        dataloaders=dataloaders,\n",
    "        logger=logger,\n",
    "        writer=writer,\n",
    "        epoch_len=epoch_len,\n",
    "        skip_oom=config.trainer.get(\"skip_oom\", True),\n",
    "    )\n",
    "\n",
    "    trainer.train()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
